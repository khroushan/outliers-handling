\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {./figs/} }

\begin{document}
%%%%%%%%%% 
\section{Introduction}

   The main focus of this study is to develop models for estimation of
   product cost. To achieve the goal we need to identify difficulties
   that can hinder the estimator to achieve a high performance,
   namely, presence of *outliers* and *novelties*. These two concepts
   prevent the model to make a good estimation either by decieving or
   by not approprietly extrapolating to all data points. What an
   outlier and a novelty have in common is that both deviate
   substantially from the common pattern of the dataset.  On the other
   hand, they are different by the process that is reponsible for
   their generation (i.e. generator). An outlier can be seen as a
   datapoint with a pure stochastic process, while novelty, as it is
   suggested by its name, is expected to have a definite process that
   is unknown or new to the user.

   To make it clear, imagine that a company introduces a new
   product. The product comes with genration of new datapoints in
   the dataset that dependeing on many factors can end up at different
   points that all previous datapoints are distrubuted in the feature
   space. Appearance of new pattern in the dataset does not
   necessarily mean that the data point is a outlier. By that we mean
   the new data point is not generated based upon a completely random
   process.

%%%%%%%%%% 
\section{Definition: Outliers vs. Novelty}

\texttt{Study last three papers and sklearn documentation}


\begin{enumerate}
\item Add a formal definition for outlier and novelty.
\item Make few actual or fictitious, relevant or errelevant examples from industry.
\item How these two concepts are different from theoritical and practical perspective?
\end{enumerate}

%% \begin{figure}
%%   \includegraphics[width=0.7\linewidth]{outlier_vs_novelty}
%% \caption{Outliers can be identified as points in the dataset that deviates from the common statistical behaviour of the dataset. They can be regarded as extra level of rendomness that can generate them but the process in itself is random. In contrast novelty are generated by a new introduced process that will be part of data but different from the majority.(left) outliers (right) novelty }
%% \end{figure}


Here we identify two specific forms of outliers:

\begin{itemize}  
\item Outliers are uniformly distributated in range of inliers
\item Outliers are isolated in the feature space
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\linewidth]{y_vs_xs}
  \caption{Example of two bi-modal dataset with two features. We can
    identify two isolated density dataset. Notice that the target
    value of outliers is a random number in the same expected
    range. This is a sensible assumption, otherwise if the target has
    a same form of functionality of the features, even out of range,
    lead to the same trained model.  }
\end{figure}


%%%%%%%%%%
\section{Approaches}
Here we take two main approches toward the problem. (i){\bf Supervised} and (ii) {\bf Unsupervised} methods.


%%%%%%%%%% 
\section{Unsupervised Approaches}
What are the advantages?

%%%%%
\subsection{Simple statistical tools}

Comparing the distance of data point to the center of data and variation of data.

%%%%%
\subsection{}

Extend the idea from previous section for a non-unimodal data distribution. In such cases the previous recipe fails. What we can do is to compare the distance with the local density.

The local density is determined by {\bf KNN}

%%%%%
\subsection{Isolation Forest}

Outliers have shorter tree branch length compare to the outliers

%%%%%%%%%% 
\section{Supervised}

%%%%%
\subsection{Robust Regression on clustered data}

The approach that we present in this section is to build a robust regression by mean of clustering. The process starts by applying a {\em proper} clustering method to the dataset.  Two different outcomes are expected:

\begin{enumerate}
\item \label{few-cluster} There are few clusters that contain the majority of the dataset.
\item \label{uniform-cluster} Dataset is distributed among clusters almost uniformly.
\end{enumerate}

If the outcome \ref{few-cluster}  happens, the regression model of interest will be trained on the sorted clusters based on their population from large to small. The test error will be measured every time a new cluster is added to the training dataset. The accumulated clusters with minimum generalization error will be the final dataset to train a regression model.

If the outcome \ref{uniform-cluster}  happens, then there would be no preferance among the clusters to start with. The best course of action would be similar to {\em cross validation} by taking one cluster  out and train on the rests. The best model can be reached either by taking average of parameters if a model is linear,  or exclude the cluster that when it is added to the training dataset increase the generalization error dramaticly. The later approach is tacken when avering of parameters does not make any sense such as case of decision tree. 

(number of cluster is large to make sure that the elbow criteria is passed).


\subsection{Curse of Dimensionality}
When the number of features is too large, the clustering method lose its reliability.


\subsection{Algorithm}
\begin{itemize}
\item Clustering with large number of clusters
\item Apply clustering, make sure you pass elbow criteria.
\item Sort cluster based on their population.
\item Start training from high population accumulatively.
\item Measure generalization error.
\item Plot error agains clusters.
\end{itemize}

%%%%%%%%%%
\section{Summary}


%%%%%%%%%% 
\begin{thebibliography}{10}

\bibitem{lof} Breunig, Markus M., et al. ``LOF: identifying density-based local outliers.'' {\em Proceedings of the 2000 ACM SIGMOD international conference on Management of data.} 2000.
  
\bibitem{iso_forest} Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. ``Isolation forest.'' {\em 2008 eighth ieee international conference on data mining. IEEE,} 2008.

\bibitem{high_dim} Sch\"{o}lkopf, Bernhard, et al. ``Estimating the support of a high-dimensional distribution.'' {\em Neural computation 13.7} (2001): 1443-1471.

\bibitem{outlier_survey}  Hodge, Victoria, and Jim Austin. ``A survey of outlier detection methodologies.'' {\em Artificial intelligence review 22.2} (2004): 85-126.

\bibitem{advers_novelty_detection} Sabokrou, Mohammad, et al. ``Adversarially learned one-class classifier for novelty detection.'' {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}. 2018.

\bibitem{rare_event} Carreno, Ander, Inaki Inza, and Jose A. Lozano. ``Analyzing rare event, anomaly, novelty and outlier detection terms under the supervised classification framework.'' {\em Artificial Intelligence Review} (2019): 1-20.
\end{thebibliography}

\end{document}
